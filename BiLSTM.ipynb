{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time, random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from lstm import LSTMSentiment\n",
    "from bilstm import BiLSTMSentiment\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    return accuracy_score(truth,pred)\n",
    "\n",
    "def get_f1(truth,pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    return f1_score(truth,pred,average='weighted')\n",
    "\n",
    "def train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in tqdm(train_iter, desc='Train epoch '+str(epoch+1)):\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    f1 = get_f1(truth_res, pred_res)\n",
    "    return avg_loss, acc,f1\n",
    "\n",
    "\n",
    "def train_epoch(model, train_iter, loss_function, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in train_iter:\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    f1 = get_f1(truth_res, pred_res)\n",
    "    return avg_loss, acc,f1\n",
    "\n",
    "\n",
    "def evaluate(model, data, loss_function, name):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    for batch in data:\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    f1 = get_f1(truth_res, pred_res)\n",
    "    print(name + ': loss %.2f acc %.1f f1 %.2f' % (avg_loss, acc*100,f1))\n",
    "    return acc,f1\n",
    "\n",
    "\n",
    "def load_sst(text_field, label_field, batch_size):\n",
    "    train, dev, test = data.TabularDataset.splits(path='./data/SST2/', train='train.csv',\n",
    "                                                  validation='dev.csv', test='test.csv', format='csv',\n",
    "                                                  fields=[('text', text_field), ('label', label_field)])\n",
    "    text_field.build_vocab(train, dev, test)\n",
    "    label_field.build_vocab(train, dev, test)\n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test),\n",
    "                batch_sizes=(batch_size, len(dev), len(test)), sort_key=lambda x: len(x.text), repeat=False, device=-1)\n",
    "    return train_iter, dev_iter, test_iter\n",
    "\n",
    "\n",
    "# def adjust_learning_rate(learning_rate, optimizer, epoch):\n",
    "#     lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr\n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "timestamp = str(int(time.time()))\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "train_iter, dev_iter, test_iter = load_sst(text_field, label_field, BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freqs': Counter({'0': 19347,\n",
       "          '1': 9808,\n",
       "          '2': 730,\n",
       "          '3': 153,\n",
       "          '4': 906,\n",
       "          '5': 1187,\n",
       "          'label': 3}),\n",
       " 'itos': ['<unk>', '0', '1', '5', '4', '2', '3', 'label'],\n",
       " 'stoi': defaultdict(<function torchtext.vocab._default_unk_index>,\n",
       "             {'0': 1,\n",
       "              '1': 2,\n",
       "              '2': 5,\n",
       "              '3': 6,\n",
       "              '4': 4,\n",
       "              '5': 3,\n",
       "              '<unk>': 0,\n",
       "              'label': 7}),\n",
       " 'vectors': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(label_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji dictionary load successfully\n",
      "word2vec load successfully\n",
      "emoji2vec load successfully\n",
      "emoticon2vec load successfully\n",
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "#load dictionary\n",
    "import pickle\n",
    "emoji_dict =pickle.load(open('./data/SST2/emoji_dict.p','rb'))\n",
    "print(\"emoji dictionary load successfully\")\n",
    "# load embedding\n",
    "word_to_idx = text_field.vocab.stoi\n",
    "pretrained_embeddings = np.random.uniform(-0.25, 0.25, (len(text_field.vocab), 300))\n",
    "pretrained_embeddings[0] = 0\n",
    "#word2vec = load_bin_vec('./data/GoogleNews-vectors-negative300.bin', word_to_idx)\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word2vec= KeyedVectors.load_word2vec_format('./embedding/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "for word in word2vec.vocab:\n",
    "    pretrained_embeddings[word_to_idx[word]-1] = word2vec[word]\n",
    "print(\"word2vec load successfully\")\n",
    "emoji2vec =KeyedVectors.load_word2vec_format('./embedding/emoji2vec.bin', binary=True)\n",
    "for word in emoji2vec.vocab:\n",
    "    if word in emoji_dict.keys():\n",
    "        pretrained_embeddings[word_to_idx[emoji_dict[word]]-1] = emoji2vec[word]\n",
    "print(\"emoji2vec load successfully\")\n",
    "emoticon2vec = KeyedVectors.load_word2vec_format('./embedding/emoticon2vec.txt', binary=False)\n",
    "\n",
    "\n",
    "\n",
    "for word in emoticon2vec.vocab:\n",
    "    pretrained_embeddings[word_to_idx[word]-1] = emoticon2vec[word]\n",
    "print(\"emoticon2vec load successfully\")    \n",
    "print('Loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTMSentiment(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,\\\n",
    "                          use_gpu=USE_GPU, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.1125  0.1872  0.0752  ...  -0.1721  0.1176  0.2066\n",
       " 0.0801  0.1050  0.0498  ...   0.0037  0.0476 -0.0688\n",
       "          ...             â‹±             ...          \n",
       " 0.2044  0.1121  0.2245  ...  -0.2238 -0.1412 -0.0477\n",
       " 0.1662  0.0474  0.2463  ...   0.1013 -0.1395 -0.0925\n",
       "-0.0400  0.0009 -0.0988  ...   0.0423 -0.0371 -0.0146\n",
       "[torch.FloatTensor of size 51322x300]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define optimizer and loss function\n",
    "best_model = model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 1:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Writing to /Users/nihaozheng/Desktop/NLP/project/model/pytorch-sentiment-classification-master/runs/1523740546\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nihaozheng/Desktop/NLP/project/model/pytorch-sentiment-classification-master/bilstm.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs = F.log_softmax(y)\n",
      "Train epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [1:11:53<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.84 acc 67.1 f1 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nihaozheng/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:89: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/Users/nihaozheng/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: loss 1.30 acc 38.2 f1 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 2:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 1.41 acc 32.0 f1 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [2:25:52<00:00,  2.16s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.41 acc 85.9 f1 0.854\n",
      "Dev: loss 0.95 acc 57.9 f1 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 3:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 1.03 acc 50.4 f1 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [2:04:25<00:00,  1.84s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.21 acc 92.0 f1 0.919\n",
      "Dev: loss 0.85 acc 67.6 f1 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 4:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 0.92 acc 63.7 f1 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [2:25:24<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.14 acc 94.0 f1 0.940\n",
      "Dev: loss 0.76 acc 72.1 f1 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 5:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 0.78 acc 70.4 f1 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [3:04:34<00:00,  2.74s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.11 acc 94.7 f1 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 6:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: loss 0.88 acc 70.1 f1 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [3:51:30<00:00,  3.43s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.09 acc 94.9 f1 0.949\n",
      "Dev: loss 0.97 acc 74.1 f1 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 7:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 0.99 acc 71.7 f1 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [2:12:09<00:00,  1.96s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.09 acc 95.3 f1 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 8:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: loss 0.99 acc 73.9 f1 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [1:55:39<00:00,  1.71s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.08 acc 95.5 f1 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 9:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: loss 1.06 acc 73.8 f1 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [1:55:49<00:00,  1.72s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.08 acc 95.3 f1 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 10:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: loss 1.02 acc 74.0 f1 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [1:02:16<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.08 acc 95.6 f1 0.956\n",
      "Dev: loss 1.06 acc 72.3 f1 0.73\n",
      "Final Test: loss 1.12 acc 68.8 f1 0.70\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss, acc,f1 = train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\n",
    "    tqdm.write('Train: loss %.2f acc %.1f f1 %.3f' % (avg_loss, acc*100,f1))\n",
    "    dev_acc , dev_f1= evaluate(model, dev_iter, loss_function, 'Dev')\n",
    "    if dev_acc > best_dev_acc:\n",
    "        if best_dev_acc > 0:\n",
    "            os.system('rm '+ out_dir + '/best_model' + '.pth')\n",
    "        best_dev_acc = dev_acc\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), out_dir + '/best_model' + '.pth')\n",
    "        # evaluate on test with the best dev performance model\n",
    "        test_acc,test_f1 = evaluate(best_model, test_iter, loss_function, 'Test')\n",
    "test_acc,test_f1 = evaluate(best_model, test_iter, loss_function, 'Final Test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# try different dim of hidden layer (after load dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "timestamp = str(int(time.time()))\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "train_iter, dev_iter, test_iter = load_sst(text_field, label_field, BATCH_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LSTMSentiment(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,\\\n",
    "                          use_gpu=USE_GPU, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "-0.1564  0.1313 -0.0573  ...   0.2062 -0.0458  0.1574\n",
       " 0.0801  0.1050  0.0498  ...   0.0037  0.0476 -0.0688\n",
       "          ...             â‹±             ...          \n",
       " 0.1075  0.0449  0.2172  ...  -0.1078  0.1145  0.2146\n",
       "-0.2470 -0.1418 -0.0502  ...  -0.0088 -0.0527 -0.1548\n",
       "-0.0400  0.0009 -0.0988  ...   0.0423 -0.0371 -0.0146\n",
       "[torch.FloatTensor of size 51322x300]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define optimizer and loss function\n",
    "best_model = model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 1:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Writing to /Users/nihaozheng/Desktop/NLP/project/model/pytorch-sentiment-classification-master/runs/1523730993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nihaozheng/Desktop/NLP/project/model/pytorch-sentiment-classification-master/lstm.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs = F.log_softmax(y)\n",
      "Train epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [42:42<00:00,  1.58it/s]\n",
      "/Users/nihaozheng/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.89 acc 65.0 f1 0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nihaozheng/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:89: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: loss 1.15 acc 65.0 f1 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 2:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 1.20 acc 62.5 f1 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [1:14:23<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.48 acc 83.4 f1 0.822\n",
      "Dev: loss 0.87 acc 67.3 f1 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 3:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 0.94 acc 64.7 f1 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2130/4049 [37:10<33:29,  1.05s/it] "
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss, acc,f1 = train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\n",
    "    tqdm.write('Train: loss %.2f acc %.1f f1 %.3f' % (avg_loss, acc*100,f1))\n",
    "    dev_acc , dev_f1= evaluate(model, dev_iter, loss_function, 'Dev')\n",
    "    if dev_acc > best_dev_acc:\n",
    "        if best_dev_acc > 0:\n",
    "            os.system('rm '+ out_dir + '/best_model' + '.pth')\n",
    "        best_dev_acc = dev_acc\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), out_dir + '/best_model' + '.pth')\n",
    "        # evaluate on test with the best dev performance model\n",
    "        test_acc,test_f1 = evaluate(best_model, test_iter, loss_function, 'Test')\n",
    "test_acc,test_f1 = evaluate(best_model, test_iter, loss_function, 'Final Test')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# try Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "timestamp = str(int(time.time()))\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "train_iter, dev_iter, test_iter = load_sst(text_field, label_field, BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTMSentiment(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,\\\n",
    "                          use_gpu=USE_GPU, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.0228 -0.1420 -0.0043  ...   0.1172  0.0888  0.1090\n",
       " 0.0801  0.1050  0.0498  ...   0.0037  0.0476 -0.0688\n",
       "          ...             â‹±             ...          \n",
       "-0.0129 -0.0334  0.0001  ...   0.1511  0.0820 -0.0477\n",
       " 0.0962  0.1528  0.1690  ...   0.0341  0.2265  0.2421\n",
       "-0.0400  0.0009 -0.0988  ...   0.0423 -0.0371 -0.0146\n",
       "[torch.FloatTensor of size 51322x300]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define optimizer and loss function\n",
    "# with smaller lr and add regularizaition\n",
    "best_model = model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3,weight_decay=1e-5)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 1:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Writing to /Users/nihaozheng/Desktop/NLP/project/model/pytorch-sentiment-classification-master/runs/1523837993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nihaozheng/Desktop/NLP/project/model/pytorch-sentiment-classification-master/bilstm.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs = F.log_softmax(y)\n",
      "Train epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [2:19:52<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.87 acc 65.6 f1 0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nihaozheng/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:89: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/Users/nihaozheng/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev: loss 0.89 acc 68.1 f1 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 2:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 0.93 acc 66.0 f1 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4049/4049 [5:40:55<00:00,  5.05s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 0.48 acc 83.8 f1 0.831\n",
      "Dev: loss 0.77 acc 69.6 f1 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train epoch 3:   0%|          | 0/4049 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 0.77 acc 70.0 f1 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2869/4049 [2:13:05<54:44,  2.78s/it]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2880\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2881\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2882\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-8e7c02a99276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train: loss %.2f acc %.1f f1 %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-64bbd2a7d077>\u001b[0m in \u001b[0;36mtrain_epoch_progress\u001b[0;34m(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   1824\u001b[0m                                             value, tb, tb_offset=tb_offset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1836\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, exc_tuple)\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \"\"\"\n\u001b[1;32m   1780\u001b[0m         \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_exc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mformat_exception_only\u001b[0;34m(etype, value)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTracebackException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, _seen)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36m_load_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36m_load_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36m_load_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;34m\"\"\"Private API. force all lines in the stack to be loaded.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mgetline\u001b[0;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mgetlines\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdatecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMemoryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mclearcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mupdatecache\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextIOWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_buffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nihaozheng/anaconda/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss, acc,f1 = train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\n",
    "    tqdm.write('Train: loss %.2f acc %.1f f1 %.3f' % (avg_loss, acc*100,f1))\n",
    "    dev_acc , dev_f1= evaluate(model, dev_iter, loss_function, 'Dev')\n",
    "    if dev_acc > best_dev_acc:\n",
    "        if best_dev_acc > 0:\n",
    "            os.system('rm '+ out_dir + '/best_model' + '.pth')\n",
    "        best_dev_acc = dev_acc\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), out_dir + '/best_model' + '.pth')\n",
    "        # evaluate on test with the best dev performance model\n",
    "        test_acc,test_f1 = evaluate(best_model, test_iter, loss_function, 'Test')\n",
    "test_acc,test_f1 = evaluate(best_model, test_iter, loss_function, 'Final Test')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
